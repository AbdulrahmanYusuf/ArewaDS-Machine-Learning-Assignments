{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install gymnasium[classic-control]\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Abdulrahman Yusuf\\miniconda3\\envs\\dscience\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "c:\\Users\\Abdulrahman Yusuf\\miniconda3\\envs\\dscience\\lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:180: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "   env.render()\n",
    "   env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "       observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04541239  0.15727599 -0.01231819 -0.29397458] -> 1.0\n",
      "[ 0.04855791 -0.03766819 -0.01819769 -0.00520194] -> 1.0\n",
      "[ 0.04780455  0.15770994 -0.01830172 -0.30357045] -> 1.0\n",
      "[ 0.05095874 -0.03714646 -0.02437313 -0.0167152 ] -> 1.0\n",
      "[ 0.05021581 -0.23191054 -0.02470744  0.26817918] -> 1.0\n",
      "[ 0.0455776  -0.03644486 -0.01934385 -0.03219312] -> 1.0\n",
      "[ 0.04484871  0.15894908 -0.01998772 -0.33091593] -> 1.0\n",
      "[ 0.04802769  0.35434976 -0.02660603 -0.6298344 ] -> 1.0\n",
      "[ 0.05511468  0.15960898 -0.03920272 -0.34564787] -> 1.0\n",
      "[ 0.05830686  0.355266   -0.04611568 -0.6504306 ] -> 1.0\n",
      "[ 0.06541218  0.1608157  -0.05912429 -0.3726184 ] -> 1.0\n",
      "[ 0.0686285   0.35672554 -0.06657666 -0.68334216] -> 1.0\n",
      "[ 0.07576301  0.16258815 -0.08024351 -0.41234025] -> 1.0\n",
      "[ 0.07901477  0.35875046 -0.08849031 -0.72920316] -> 1.0\n",
      "[ 0.08618978  0.5549769  -0.10307437 -1.0483733 ] -> 1.0\n",
      "[ 0.09728932  0.3613624  -0.12404184 -0.78974324] -> 1.0\n",
      "[ 0.10451657  0.16814232 -0.1398367  -0.53851485] -> 1.0\n",
      "[ 0.10787942 -0.02476542 -0.150607   -0.2929559 ] -> 1.0\n",
      "[ 0.1073841  -0.21745513 -0.15646613 -0.05130442] -> 1.0\n",
      "[ 0.103035   -0.02047606 -0.1574922  -0.3889772 ] -> 1.0\n",
      "[ 0.10262548  0.17648971 -0.16527176 -0.72688085] -> 1.0\n",
      "[ 0.10615528 -0.01600848 -0.17980938 -0.4904371 ] -> 1.0\n",
      "[ 0.1058351  -0.20819911 -0.18961811 -0.25937852] -> 1.0\n",
      "[ 0.10167112 -0.0109476  -0.19480568 -0.6053668 ] -> 1.0\n",
      "[ 0.10145217 -0.20288883 -0.20691302 -0.37981203] -> 1.0\n",
      "[ 0.09739439 -0.3945643  -0.21450926 -0.15883084] -> 1.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"{observation} -> {reward}\")\n",
    "\n",
    "   \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAIRNESS IN MACHINE LEARNING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition:\n",
    "    Fairnes in machine learning can be considered as an ethical issues related to data, algorthms and people that may involve in the system development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias in Data and Algorithms\n",
    "   Unfairness is the introduction of any possibble bias that could affect data, algorithms and stakeholder of the system.\n",
    "   It could be done intentional or unintentional\n",
    "   It is considered as a socio technical challenge that may be very difficult to avoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HARMS FROM UNFARNESS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfairne means is the advance negative impacts, or \"harms\" to a group of people, such as those defined in terms of race, gender, age, or disability status."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Harm Related fairness can be classfied as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Allocation: \n",
    "    Gender or ethnicity are more likely to favored over another.\n",
    "    \n",
    "2. Quality of service:\n",
    "    the data use for training in a specific scenario may not give better results in reality due to complexity. Hence poor performance of performing service.\n",
    "\n",
    "3. Stereotyping:\n",
    "    Attribute that could lead to favourism to certain group are assigned.\n",
    "\n",
    "4. Denigration:\n",
    "    To unfairly criticize and label something or someone.\n",
    "\n",
    "5. Over- or under- representation:\n",
    "    It desctribe the idea of unequal or unfair representation of in levelling certain variable. For examples more men are ganicologist in Nigeria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESOLVING UNFAIRNESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Detecting and resolving unfainess is not an easy task, some data are originally biased. \n",
    "2. For example: Historical data, poor representation of image in people with dark-skin. \n",
    "3. improper assumption, lebeling and identification of criminals by face."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mitigating Unfairness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although, there are a number of model available with didffrent capabilities. It is left for the deveoper to explore and look for the tradeoff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be fair and avoid biases in one has to:\n",
    "explore and understand different stakeholders and their respective backgrounds;\n",
    "should be experienced data scientist;\n",
    "apply different methods for detecting and resolving possible occurance of bias. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
